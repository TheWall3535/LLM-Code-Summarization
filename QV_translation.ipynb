{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bff245d-a8da-468b-a2b7-d0cea0018cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # regular expression string search\n",
    "import tiktoken # open ai tokenizer\n",
    "import openai\n",
    "import ast # create json object from literal\n",
    "import pandas as pd \n",
    "from scipy.stats import t as t_test\n",
    "import nltk\n",
    "import numpy as np\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f6dcf6f-82e2-4127-9011-113a359262c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dictionary(dict):\n",
    "    def __init__(self):\n",
    "        self = dict()\n",
    "    def add(self, key, value):\n",
    "            self[key] = value\n",
    "\n",
    "def count_tokens(Text,MODEL):\n",
    "    encoding = tiktoken.encoding_for_model(MODEL)\n",
    "    tokens = encoding.encode(Text)\n",
    "    return len(tokens)\n",
    "\n",
    "def return_tokens(Text,MODEL):\n",
    "    encoding = tiktoken.encoding_for_model(MODEL)\n",
    "    tokens = encoding.encode(Text)\n",
    "    return tokens\n",
    "\n",
    "def read_and_clean(file):\n",
    "    \n",
    "    with open(file) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    rawCode = [i.lower() for i in lines if (i != \"\\n\" and not i.startswith(\"//\"))]\n",
    "    result_string = ''.join(rawCode)\n",
    "    \n",
    "        #disregard multiline commented sections - unfeasible to not split into chunks that divide commented sections\n",
    "    for pattern in re.findall(r'/\\*(.*?)\\*/',result_string,flags = re.DOTALL):\n",
    "        pattern = r\"/*\"+pattern+r\"*/\"\n",
    "        result_string = re.sub(re.escape(pattern), '', result_string)\n",
    "    \n",
    "    return result_string\n",
    "\n",
    "def read_file(filename):\n",
    "    with open (filename) as file:\n",
    "        rfile = file.read()\n",
    "    return rfile\n",
    "\n",
    "def make_prompts(Code_files):\n",
    "    # takes multiple files in list\n",
    "\n",
    "    sized_prompts = [\"\"]\n",
    "    index = 0\n",
    "    for file in Code_files:\n",
    "\n",
    "        runningChunkTotal = 0\n",
    "        line_counter = 0\n",
    "        \n",
    "        clean_code_sections = read_and_clean(file).split(\";\")\n",
    "        splits_to_avoid = split_index(file)\n",
    "        \n",
    "        for chunk in clean_code_sections:\n",
    "            \n",
    "            t = count_tokens(chunk, MODEL)\n",
    "            runningChunkTotal += t\n",
    "            \n",
    "            if runningChunkTotal > PROMPT_LENGTH and not line_counter - 1 in splits_to_avoid:\n",
    "                runningChunkTotal = t \n",
    "                index += 1\n",
    "                sized_prompts.append(\"\")\n",
    "                # print(runningChunkTotal)\n",
    "                sized_prompts[index] += (chunk + \"\\n\")\n",
    "            else:\n",
    "                # print(runningChunkTotal,\"HERE\")\n",
    "                sized_prompts[index] += (chunk + \"\\n\")\n",
    "                \n",
    "            line_counter += 1\n",
    "        \n",
    "\n",
    "    return sized_prompts\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1,max=60),stop=stop_after_attempt(6))\n",
    "def completion_with_backoff(**kwargs):\n",
    "    return openai.ChatCompletion.create(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb480bf0-2b75-429a-9d86-54bda5d4e0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_LENGTH = 1200\n",
    "\n",
    "params = dictionary()\n",
    "for line in read_file(\"GPT_AUTH.txt\").split(\"\\n\"):\n",
    "    sep = line.split(\" \")\n",
    "    params.add(sep[0],sep[1])\n",
    "\n",
    "AZURE_MODEL_NAME = params[\"AZURE_MODEL_NAME\"] # name of our deployment at oai.azure.com\n",
    "MODEL = params[\"MODEL\"] # actual name of the gpt model\n",
    "openai.api_base = params[\"OPENAI_API_BASE\"]\n",
    "openai.api_key = params[\"OPENAI_API_KEY\"]\n",
    "openai.api_version = params[\"OPENAI_API_VERSION\"]\n",
    "openai.api_type = params[\"API_TYPE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a0f979e7-9ad3-4948-b9f1-5733ba26c3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_index(file):\n",
    "    # takes single file as string\n",
    "    \n",
    "    clean_code_sections = read_and_clean(file).split(\"\\n\")\n",
    "    \n",
    "    # if for \n",
    "    tuples = [(n,i) for n,i in enumerate(clean_code_sections) if i.strip().startswith(\"next\") or (i.strip().startswith(\"for\") and \" = \" in i)]\n",
    "\n",
    "    starters = [\"for\"]\n",
    "    closers = [\"next\"]\n",
    "\n",
    "#     QV_exceptions = []\n",
    "\n",
    "    algo_count = 0\n",
    "    prev_i = []\n",
    "    indicies = []\n",
    "    for i,j in tuples:\n",
    "        start = re.findall('|'.join(starters),j)\n",
    "        close = re.findall('|'.join(closers),j)\n",
    "        if any(start) and any(close):\n",
    "            continue\n",
    "        elif any(start):\n",
    "            if algo_count == 0:\n",
    "                key = i\n",
    "                # print(i,j)\n",
    "            if any(prev_i):\n",
    "                # print(prev_i,start)\n",
    "                algo_count += 1\n",
    "            else:\n",
    "                algo_count+=1\n",
    "            if i > 0:\n",
    "                prev_i = start\n",
    "            # print(algo_count,i,j)\n",
    "        elif any(close):\n",
    "            algo_count -= 1\n",
    "            if algo_count == 0:\n",
    "                # print(i,j)\n",
    "                for k in range(key,i):\n",
    "                    indicies.append(k)\n",
    "            if i > 0:\n",
    "                prev_i = start\n",
    "            # print(algo_count,i,j)\n",
    "            \n",
    "    return indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4442ddd2-e21f-437e-ab25-93e8ba9c94b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Code_files = [\"BradySales_DATA.txt\"]\n",
    "\n",
    "# example_input_SQL = read_file(\"Examples/SqlClassCompressed.txt\")\n",
    "# accepted_output_SQL = read_file(\"Examples/SqlClassExample.txt\")\n",
    "\n",
    "t = make_prompts(Code_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219838d3-2361-404d-8f06-632af336ef03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Chunk = t[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b3b5a6-f4ca-47fe-bd50-3190fff16bbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response_2 = completion_with_backoff(\n",
    "    engine = AZURE_MODEL_NAME,\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\" : \"You are giving step by step low level detail of Qlikview codes so that developers will not have to reference the source code after reading your response.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Be extremely specific about any conditions that are in the code. \\n code:\\n```\\n {example_code} \\n```\\n\"},\n",
    "            {\"role\": \"assistant\", \"content\" : f\"{example_translation}\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Be extremely specific about any conditions that are in the code.  \\n code:\\n```\\n {Chunk} \\n```\\n\"}\n",
    "                ], \n",
    "\n",
    "    # max_tokens = 2000\n",
    "    max_tokens = 16000 - (count_tokens(Chunk, MODEL)  + count_tokens(example_code,MODEL) + count_tokens(example_translation,MODEL)),\n",
    "    temperature = 1.0, #introducing a higher degree of randomness into the output, used with the n parameter to aid in getting consistently good outputs for us\n",
    "    logit_bias = logit_bias,\n",
    "    n = 3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaceaac-1239-4b95-89bf-6b635b523705",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_3 = completion_with_backoff(\n",
    "    engine = AZURE_MODEL_NAME,\n",
    "    messages = [\n",
    "            {\"role\": \"user\", \"content\": f\"Use the following summary of SAS code to generate a script of SAS code WITHOUT COMMENTS IN IT. Return only the code. For reference, this code does the following:{(', ').join(example_columns)}. \\n summary:\\n```\\n {summary} \\n```\\n\"},\n",
    "                ],\n",
    "    max_tokens = PROMPT_LENGTH+300,\n",
    "    temperature = 0,\n",
    "    n = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35089b9f-f78a-4b22-971d-c3d052642d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = nltk.wordpunct_tokenize(read_file(\"Candidate.txt\"))\n",
    "candidate = nltk.wordpunct_tokenize(response_3[\"choices\"][0][\"message\"][\"content\"].lower())\n",
    "score = sentence_bleu([reference], candidate)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f31376-9894-44c9-8ce8-c45cb602ba44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the chosen summary and write to a file\n",
    "# update an ongoing summary up to current chunk (make sure num tokens in this plus num tokens in prompt < 16k)\n",
    "# in messages, pass the summaries of the most recent chunks\n",
    "# clean out the comments in the SAS file\n",
    "# \n",
    "\n",
    "# reduce n in the summarization\n",
    "\n",
    "# whats relationship between number n needed for good summary and quality of examples used in the prompts\n",
    "        # thoughts - if we have an app that gets used by devs we could collect ground truth over time\n",
    "        # parameterized n for people to change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6007ead5-f59f-4590-83ab-81eeb234128d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.encoding_for_model(MODEL)\n",
    "encoding.decode_single_token_bytes(52340)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m110",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m110"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
